{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNi4oP655X6LxHoz+lsD3Cp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisW2420/FedPKDG/blob/main/FedPKDG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FedPKDG -- Prune + KD + GAN + FL\n",
        "This prototype implements the algorithm in a distributed setting\n",
        "TODO:\n",
        "1. implement a FedAvg aggregator/server\n",
        "2. build a centralised FL system with n clients connected to the server\n",
        "3. design experiments to assess accuracy, efficiency, generalisation on homogenoeous data\n",
        "4. repeat experiments on heterogeneous data, identical model sparsity\n",
        "5. repeat experiments on heterogeneous data, different model sparsity, mimicing different computational capability of clients"
      ],
      "metadata": {
        "id": "GR5F2i850OT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "0qqRJILKrnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: package versions are very important\n",
        "!pip install -q tensorflow-model-optimization # for pruning\n",
        "!pip install -q git+https://github.com/tensorflow/docs # newest tf\n",
        "!pip install --upgrade keras #newest keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgElPEcBrp3j",
        "outputId": "b21228ad-c727-4063-8140-654a072d5fb6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/242.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 versions of keras are used for different functionalities, imported as different names\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "import tf_keras as keras_model #only for pruning\n",
        "from tf_keras import layers as model_layers\n",
        "import keras\n",
        "import tempfile\n",
        "from tf_keras.callbacks import EarlyStopping, Callback\n",
        "from keras import ops, layers\n",
        "from tensorflow_docs.vis import embed # for GAN\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0uN9jtQssMKu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging metrics with WandB\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "from wandb.keras import WandbMetricsLogger"
      ],
      "metadata": {
        "id": "yLSfMJe-spr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "lDJsxBBKO5k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST\n",
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "S5AEP2kXO7Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1e86cf-b33d-4b47-f91d-149f17497f57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components Implementation"
      ],
      "metadata": {
        "id": "UDusEhV8rad4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model zoo"
      ],
      "metadata": {
        "id": "yivLHBBVKZ7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "LiDzieUBHTED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def miniCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"minicnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def smallCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"smallcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def mediumCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"mediumcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def bigCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"bigcnn\",\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZU1h8xrMKcWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN"
      ],
      "metadata": {
        "id": "2Yeo7R7OHVFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128\n",
        "\n",
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "\n",
        "# Create the discriminator.\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((generator_in_channels,)),\n",
        "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "        # 7x7x(128 + num_classes) map.\n",
        "        layers.Dense(7 * 7 * generator_in_channels),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Reshape((7, 7, generator_in_channels)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZvYMNrI8HWna"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN (don't run for now)"
      ],
      "metadata": {
        "id": "zvdZfOosKmTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# very unecessary in this class itself, might be useful to combined pruning and KD and ordinary training\n",
        "class CNN(): #keras_model.Model\n",
        "  def __init__(self, config_name, **kwargs):\n",
        "    #super(CNN, self).__init__(**kwargs)\n",
        "    if config_name == 'small':\n",
        "      self.model = smallCNN()\n",
        "    elif config_name == 'medium':\n",
        "      self.model = mediumCNN()\n",
        "    elif config_name == 'big':\n",
        "      self.model = bigCNN()\n",
        "    else:\n",
        "      print('default model of medium CNN')\n",
        "      self.model = mediumCNN()\n",
        "    self.optimizer = 'adam'\n",
        "    self.task_loss = keras_model.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.metricslist = [keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "    self.validation_split = 0.1\n",
        "    self.early_stopping = EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      min_delta=0.001,  # only consider as improvement significant changes\n",
        "      patience=2,      # number of epochs with no improvement after which training will be stopped\n",
        "      verbose=1,\n",
        "      mode='min'        # 'min' because we want to minimize the loss\n",
        "    )\n",
        "    self.callbacks = []\n",
        "\n",
        "\n",
        "  def train(self, training_data, testing_data = None, epochs = 10, is_earlystop = True, **kwargs):\n",
        "    #super(CNN, self).compile(optimizer=self.optimizer, metrics=self.metricslist, **kwargs)\n",
        "    self.model.compile(\n",
        "      optimizer=self.optimizer,\n",
        "      loss=self.task_loss,\n",
        "      metrics=self.metricslist\n",
        "    )\n",
        "    if is_earlystop and self.early_stopping not in self.callbacks:\n",
        "      self.callbacks.append(self.early_stopping)\n",
        "    x_train, y_train = training_data\n",
        "    self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,validation_split=self.validation_split, callbacks=self.callbacks)\n",
        "    if testing_data:\n",
        "      x_test, y_test = testing_data\n",
        "      self.model.evaluate(x_test, y_test)\n",
        "    return self.model\n"
      ],
      "metadata": {
        "id": "buLK7EZXKkBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN"
      ],
      "metadata": {
        "id": "C9du692bJ1GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = ops.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = ops.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = ops.shape(real_images)[0]\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = ops.concatenate(\n",
        "            [generated_images, image_one_hot_labels], -1\n",
        "        )\n",
        "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = ops.concatenate(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = ops.concatenate(\n",
        "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = ops.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = ops.concatenate(\n",
        "                [fake_images, image_one_hot_labels], -1\n",
        "            )\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "dimSAKxdJ9jq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image generation functions\n",
        "def generate_image(generator, target_class, latent_dim):\n",
        "    noise_matrix = keras.random.normal(shape=(1, latent_dim))\n",
        "    # Convert the target label to one-hot encoded vectors.\n",
        "    target_label = keras.utils.to_categorical([target_class], num_classes)\n",
        "    target_label = ops.cast(target_label, \"float32\")\n",
        "    noise_and_labels = ops.concatenate([noise_matrix, target_label], 1)\n",
        "    fake = generator.predict(noise_and_labels,verbose = 0)\n",
        "    return fake\n",
        "\n",
        "def pseudoDataset(generator, total_num, latent_dim): # producing equal numbers of samples for each class\n",
        "    pseudo_images = []\n",
        "    for num in range(10):\n",
        "      target_class = num\n",
        "      print('Generating', int(total_num/10), 'fake images of digit', num, '......')\n",
        "      for _ in range(int(total_num/10)):\n",
        "        generated_images = generate_image(target_class, latent_dim)\n",
        "        generated_images *= 255.0\n",
        "        converted_images = generated_images.astype(np.uint8)\n",
        "        converted_images = ops.image.resize(converted_images, (28, 28)).numpy().astype(np.uint8)\n",
        "        pseudo_images.append(converted_images)\n",
        "    pseudo_images = np.concatenate(pseudo_images, axis=0)\n",
        "    pseudo_labels = np.repeat(np.arange(10), int(total_num/10))\n",
        "    return pseudo_images, pseudo_labels"
      ],
      "metadata": {
        "id": "IYYsK1c3nEry"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning"
      ],
      "metadata": {
        "id": "EZI_-kzpJ4OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_finetrain(base_model, _epochs, x, y, validation_split=0.1, target_sparsity = 0.5, fine_tune_epochs=0):\n",
        "  callbacks = [\n",
        "      sparsity.UpdatePruningStep(),\n",
        "      early_stopping\n",
        "  ]\n",
        "  steps_per_epoch = len(x)*(1-validation_split) // batch_size\n",
        "  begin_step=int(steps_per_epoch*fine_tune_epochs)\n",
        "  end_step=int(steps_per_epoch*_epochs)+1\n",
        "  print('begin_step=', begin_step, 'end_step=', end_step)\n",
        "  pruning_schedule = sparsity.PolynomialDecay(initial_sparsity=0, final_sparsity=target_sparsity,\n",
        "                                              begin_step=begin_step, end_step=end_step) # TODO: tune begin_step, consider fining training before starting to prune\n",
        "\n",
        "  model_for_pruning = sparsity.prune_low_magnitude(base_model, pruning_schedule=pruning_schedule) #default constant sparsity of 50%\n",
        "\n",
        "  model_for_pruning.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "  )\n",
        "\n",
        "  model_for_pruning.fit(\n",
        "      x,\n",
        "      y,\n",
        "      batch_size=batch_size,\n",
        "      validation_split=validation_split,\n",
        "      callbacks=callbacks,\n",
        "      epochs=_epochs,\n",
        "  )\n",
        "  pruned_model = sparsity.strip_pruning(model_for_pruning)\n",
        "\n",
        "  return pruned_model\n",
        "\n",
        "\n",
        "# Model size metrics\n",
        "\n",
        "def get_model_sparsity(model):\n",
        "    total_weights = 0\n",
        "    zero_weights = 0\n",
        "    for weight in model.get_weights():\n",
        "        total_weights += weight.size\n",
        "        zero_weights += np.count_nonzero(weight == 0)\n",
        "    return zero_weights / total_weights\n",
        "\n",
        "def get_gzipped_model_size(model):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, keras_file = tempfile.mkstemp('.h5')\n",
        "  model.save(keras_file, include_optimizer=False)\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(keras_file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "oxgClZVAJ-Y2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation"
      ],
      "metadata": {
        "id": "m0uflogyJ53G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distiller class from before\n",
        "class Distiller(keras_model.Model):\n",
        "    def __init__(self, teacher, student, alpha=0.1, temperature=3, **kwargs):\n",
        "        super(Distiller, self).__init__(**kwargs)\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "\n",
        "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha, temperature, **kwargs):\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=self.student_loss_fn)\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher with no gradient tracking\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        print('testing kd train step')\n",
        "        print(teacher_predictions)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of the student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Calculate the task-specific loss\n",
        "            task_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # Calculate the soft targets and the distillation loss\n",
        "            soft_targets = tf.nn.softmax(teacher_predictions / self.temperature)\n",
        "\n",
        "            print('soft target:', soft_targets)\n",
        "            student_soft = tf.nn.softmax(student_predictions / self.temperature)\n",
        "            distillation_loss = self.distillation_loss_fn(soft_targets, student_soft)\n",
        "\n",
        "            # Calculate the total loss\n",
        "            total_loss = (1 - self.alpha) * task_loss + self.alpha * distillation_loss * (self.temperature ** 2)\n",
        "            print('total_loss:', total_loss)\n",
        "\n",
        "        # Compute gradients and update weights\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(total_loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"task_loss\": task_loss, \"distillation_loss\": distillation_loss, \"total_loss\": total_loss})\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of the student\n",
        "        y_pred = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the task-specific loss\n",
        "        task_loss = self.student_loss_fn(y, y_pred)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call_model(self):\n",
        "      return self.student"
      ],
      "metadata": {
        "id": "BNTcgLrs0PmV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = mediumCNN()\n",
        "teacher.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "teacher.fit(x_train, y_train, epochs=10,validation_split=0.1, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qd4C7ndmMFi",
        "outputId": "a550673e-291f-4991-fe8a-6f26ae51cb92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 65s 11ms/step - loss: 0.2946 - sparse_categorical_accuracy: 0.9080 - val_loss: 0.0943 - val_sparse_categorical_accuracy: 0.9730\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 17s 10ms/step - loss: 0.1005 - sparse_categorical_accuracy: 0.9683 - val_loss: 0.0713 - val_sparse_categorical_accuracy: 0.9795\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 17s 10ms/step - loss: 0.0779 - sparse_categorical_accuracy: 0.9755 - val_loss: 0.0683 - val_sparse_categorical_accuracy: 0.9805\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 21s 13ms/step - loss: 0.0684 - sparse_categorical_accuracy: 0.9782 - val_loss: 0.0567 - val_sparse_categorical_accuracy: 0.9850\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 17s 10ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.0537 - val_sparse_categorical_accuracy: 0.9862\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.0544 - sparse_categorical_accuracy: 0.9823 - val_loss: 0.0497 - val_sparse_categorical_accuracy: 0.9868\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.0505 - sparse_categorical_accuracy: 0.9838 - val_loss: 0.0492 - val_sparse_categorical_accuracy: 0.9858\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 17s 10ms/step - loss: 0.0455 - sparse_categorical_accuracy: 0.9852 - val_loss: 0.0465 - val_sparse_categorical_accuracy: 0.9863\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.0427 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0516 - val_sparse_categorical_accuracy: 0.9855\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 20s 12ms/step - loss: 0.0415 - sparse_categorical_accuracy: 0.9865 - val_loss: 0.0556 - val_sparse_categorical_accuracy: 0.9833\n",
            "Epoch 10: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7cbb5bfadbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = miniCNN()\n",
        "raw.compile(optimizer=keras_model.optimizers.Adam(),\n",
        "            metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "            loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
        "raw.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
        "raw.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYBjFTE1gomK",
        "outputId": "3957929a-bc24-4c32-b01b-851e03fc6d37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1688/1688 [==============================] - 9s 4ms/step - loss: 0.3918 - sparse_categorical_accuracy: 0.8894 - val_loss: 0.2513 - val_sparse_categorical_accuracy: 0.9267\n",
            "Epoch 2/3\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2994 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.2327 - val_sparse_categorical_accuracy: 0.9362\n",
            "Epoch 3/3\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2894 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.2315 - val_sparse_categorical_accuracy: 0.9380\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.9216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27132585644721985, 0.9215999841690063]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using class above\n",
        "student = miniCNN()\n",
        "\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras_model.optimizers.Adam(),\n",
        "    metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras_model.losses.KLDivergence(),\n",
        "    alpha=0.4,\n",
        "    temperature=3,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "\n",
        "distiller.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ewKSRhhLJK",
        "outputId": "78a36478-6fd2-463c-f01c-035f0b855ad5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "testing kd train step\n",
            "Tensor(\"mediumcnn/dense_1/BiasAdd:0\", shape=(None, 10), dtype=float32)\n",
            "soft target: Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "total_loss: Tensor(\"add:0\", shape=(), dtype=float32)\n",
            "testing kd train step\n",
            "Tensor(\"mediumcnn/dense_1/BiasAdd:0\", shape=(None, 10), dtype=float32)\n",
            "soft target: Tensor(\"Softmax:0\", shape=(None, 10), dtype=float32)\n",
            "total_loss: Tensor(\"add:0\", shape=(), dtype=float32)\n",
            "1688/1688 [==============================] - 15s 7ms/step - sparse_categorical_accuracy: 0.8799 - task_loss: 0.4605 - distillation_loss: 0.3362 - total_loss: 1.4865 - val_sparse_categorical_accuracy: 0.9262\n",
            "Epoch 2/3\n",
            "1688/1688 [==============================] - 12s 7ms/step - sparse_categorical_accuracy: 0.9077 - task_loss: 0.3734 - distillation_loss: 0.2436 - total_loss: 1.1009 - val_sparse_categorical_accuracy: 0.9265\n",
            "Epoch 3/3\n",
            "1688/1688 [==============================] - 13s 7ms/step - sparse_categorical_accuracy: 0.9104 - task_loss: 0.3622 - distillation_loss: 0.2376 - total_loss: 1.0725 - val_sparse_categorical_accuracy: 0.9287\n",
            "313/313 [==============================] - 1s 2ms/step - sparse_categorical_accuracy: 0.9179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.917900025844574"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using class below\n",
        "student = miniCNN()\n",
        "\n",
        "distiller = Distiller(student=student, teacher=teacher)\n",
        "distiller.compile(\n",
        "    optimizer=keras_model.optimizers.Adam(),\n",
        "    metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=keras_model.losses.KLDivergence(),\n",
        "    alpha=0.4,\n",
        "    temperature=3,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "\n",
        "distiller.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
        "distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lvOWFg0hkbC",
        "outputId": "0143b0da-7b98-442f-abd5-91ace6020385"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "1688/1688 [==============================] - 16s 9ms/step - distillation_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9205 - val_distillation_loss: 0.2227\n",
            "Epoch 2/3\n",
            "1688/1688 [==============================] - 16s 9ms/step - distillation_loss: 0.2435 - val_sparse_categorical_accuracy: 0.9280 - val_distillation_loss: 0.2102\n",
            "Epoch 3/3\n",
            "1688/1688 [==============================] - 13s 8ms/step - distillation_loss: 0.2375 - val_sparse_categorical_accuracy: 0.9247 - val_distillation_loss: 0.2115\n",
            "313/313 [==============================] - 3s 8ms/step - sparse_categorical_accuracy: 0.9129 - distillation_loss: 0.2292\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9128999710083008, 0.2291950285434723]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copied\n",
        "class Distiller(keras_model.Model):\n",
        "    def __init__(self, teacher, student, alpha=0.1, temperature=3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"distillation_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        metrics = super().metrics\n",
        "        metrics.append(self.loss_tracker)\n",
        "        return metrics\n",
        "\n",
        "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha, temperature, **kwargs):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher with no gradient tracking\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        print('testing kd train step')\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of the student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Calculate the task-specific loss\n",
        "            task_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # Calculate the soft targets and the distillation loss\n",
        "            soft_targets = tf.nn.softmax(teacher_predictions / self.temperature, axis=1)\n",
        "            student_soft = tf.nn.softmax(student_predictions / self.temperature, axis=1)\n",
        "            distillation_loss = self.distillation_loss_fn(soft_targets, student_soft)\n",
        "\n",
        "            # Calculate the total loss\n",
        "            total_loss = (1 - self.alpha) * task_loss + self.alpha * distillation_loss * (self.temperature ** 2)\n",
        "\n",
        "        # Compute gradients and update weights\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(total_loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Report Progress\n",
        "        self.loss_tracker.update_state(distillation_loss)\n",
        "        return {\"distillation_loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward passes\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        student_predictions = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the loss\n",
        "        distillation_loss = self.distillation_loss_fn(\n",
        "            tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "            tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "        )\n",
        "        # Calculate the task-specific loss\n",
        "        task_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "        self.loss_tracker.update_state(distillation_loss)\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "xm3mI3w04Aa7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha in range(10):\n",
        "  alpha = alpha/10\n",
        "  student = miniCNN()\n",
        "  distiller = Distiller(student=student, teacher=teacher)\n",
        "  distiller.compile(\n",
        "      optimizer=keras_model.optimizers.Adam(),\n",
        "      metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "      distillation_loss_fn=keras_model.losses.KLDivergence(),\n",
        "      student_loss_fn=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      alpha=alpha,\n",
        "      temperature=3,\n",
        "  )\n",
        "  distiller.fit(x_train, y_train, epochs=3, validation_split=0.1, verbose=0)\n",
        "  print('alpha = ', alpha)\n",
        "  distiller.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IX5PJ-c4vWe",
        "outputId": "e9eac0f2-7974-4b29-f5f5-44c87d75af49"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.0\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9506 - distillation_loss: 0.1844\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.1\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9525 - distillation_loss: 0.0985\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.2\n",
            "313/313 [==============================] - 2s 5ms/step - sparse_categorical_accuracy: 0.9639 - distillation_loss: 0.0551\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.3\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9480 - distillation_loss: 0.0855\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.4\n",
            "313/313 [==============================] - 2s 5ms/step - sparse_categorical_accuracy: 0.9681 - distillation_loss: 0.0401\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.5\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9648 - distillation_loss: 0.0504\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "313/313 [==============================] - 2s 8ms/step - sparse_categorical_accuracy: 0.9564 - distillation_loss: 0.0654\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.7\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9478 - distillation_loss: 0.0795\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.8\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9634 - distillation_loss: 0.0463\n",
            "testing kd train step\n",
            "testing kd train step\n",
            "alpha =  0.9\n",
            "313/313 [==============================] - 1s 4ms/step - sparse_categorical_accuracy: 0.9557 - distillation_loss: 0.0654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw = miniCNN()\n",
        "raw.compile(optimizer=keras_model.optimizers.Adam(),\n",
        "            metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "            loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
        "raw.fit(x_train, y_train, epochs=3, validation_split=0.1)\n",
        "raw.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJipeadorz0R",
        "outputId": "4e70d489-d1e2-4cd7-9d27-e9827bd37892"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1688/1688 [==============================] - 9s 4ms/step - loss: 0.3918 - sparse_categorical_accuracy: 0.8905 - val_loss: 0.2065 - val_sparse_categorical_accuracy: 0.9427\n",
            "Epoch 2/3\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2206 - sparse_categorical_accuracy: 0.9370 - val_loss: 0.1563 - val_sparse_categorical_accuracy: 0.9627\n",
            "Epoch 3/3\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9521 - val_loss: 0.1354 - val_sparse_categorical_accuracy: 0.9640\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1502 - sparse_categorical_accuracy: 0.9568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.150151789188385, 0.9567999839782715]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function Implementation\n",
        "\n",
        "TODO:\n",
        "Dataset:\n",
        "- Dataloader\n",
        "- heterogeneous dataset partition\n",
        "- data augmentation\n",
        "\n",
        "visualisation:\n",
        "- dataset example visualisation\n",
        "- data distribution visualisation\n",
        "- confusion matrix\n",
        "-"
      ],
      "metadata": {
        "id": "rtt0HHPp6bWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_model_weights_to_zero(model):\n",
        "    for layer in model.layers:\n",
        "        zero_weights = [np.zeros_like(w) for w in layer.get_weights()]\n",
        "        layer.set_weights(zero_weights)\n",
        "    return model"
      ],
      "metadata": {
        "id": "pL7iWz7Z6ew6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def if_synced(model1, model2):\n",
        "    for layer1, layer2 in zip(model1.layers, model2.layers):\n",
        "          weights1 = layer1.get_weights()\n",
        "          weights2 = layer2.get_weights()\n",
        "          for w1, w2 in zip(weights1, weights2):\n",
        "              if not np.array_equal(w1, w2):\n",
        "                  print('different weights, syncing failed')\n",
        "    print('weights synced for client')"
      ],
      "metadata": {
        "id": "5nd729kbESwD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback zoo"
      ],
      "metadata": {
        "id": "0_Ji1-THrXgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,  # only consider as improvement significant changes\n",
        "    patience=2,      # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min'        # 'min' because we want to minimize the loss\n",
        "    )"
      ],
      "metadata": {
        "id": "9di4sjQurWkv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client"
      ],
      "metadata": {
        "id": "0XwMW_rz5C_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Client(): #TODO: add name to clients to refer to them, espeically during logging\n",
        "  def __init__(self, model_fn, x_train, y_train, generator = generator, discriminator = discriminator, **kwargs):\n",
        "    self.cnn = model_fn\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.latent_dim = 128 # hyperparam, can tune\n",
        "    self.x_private = x_train\n",
        "    self.y_private = y_train\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.validation_split=0.1\n",
        "\n",
        "  def local_train(self, epochs = 5, is_prune = False, sparsity = 0.5, fine_tune_epochs = 0, **kwargs):\n",
        "    if is_prune:\n",
        "      #!!!TODO: test prune\n",
        "      print('after aggregating client has sparsity', get_model_sparsity(client.cnn))\n",
        "      self.cnn = prune_finetrain(self.cnn, epochs, x = self.x_private, y = self.y_private, fine_tune_epochs = fine_tune_epochs) # fine_tune_epochs can take decimals, starts pruning after fine tune\n",
        "      self.cnn.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "      print('before aggregating client has sparsity', get_model_sparsity(client.cnn))\n",
        "    else:\n",
        "      self.cnn.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "      self.cnn.fit(self.x_private, self.y_private, batch_size=batch_size, epochs=epochs,validation_split=self.validation_split, callbacks=[early_stopping])\n",
        "\n",
        "  # ! TODO: cnn evaluation def eval_cnn\n",
        "\n",
        "  def train_gen(self, epochs = 20, d_learning_rate = 0.0003, g_learning_rate = 0.0003):\n",
        "    #TODO: test ConditionalGAN\n",
        "    cond_gan = ConditionalGAN(self.discriminator, self.generator, self.latent_dim)\n",
        "    cond_gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(d_learning_rate),\n",
        "        g_optimizer=keras.optimizers.Adam(g_learning_rate),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    )\n",
        "    # produce GAN training dataset\n",
        "    train_label = keras.utils.to_categorical(self.y_private, 10) # 1 hot encoding label\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((self.x_private, train_label))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    cond_gan.fit(dataset, epochs)\n",
        "\n",
        "\n",
        "  ## the following code act as interface with the server, avoid direct access to private model and dataset\n",
        "\n",
        "  def produce_logits(self, dataset): #for KD\n",
        "    #!!!TODO\n",
        "    logits = []\n",
        "    return logits\n",
        "\n",
        "  def get_cnn_weights(self): #only for FedAvg, disabled\n",
        "    return self.cnn.get_weights()\n",
        "\n",
        "  def set_cnn_weights(self, weights): #for downloading global weights\n",
        "    self.cnn.set_weights(weights)\n",
        "\n",
        "  def get_gen_weights(self): #only for FedAvg, disabled\n",
        "    return self.generator.get_weights()\n",
        "\n",
        "  def set_gen_weights(self, weights): #for downloading global weights\n",
        "    self.generator.set_weights(weights)\n",
        "\n",
        "  def get_datasize(self):\n",
        "    return self.x_private.shape[0]"
      ],
      "metadata": {
        "id": "oYQ-XPX05CW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server"
      ],
      "metadata": {
        "id": "8RMvpBJwLuju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Server():\n",
        "  def __init__(self, model_fn, client_list, generator = generator, comm_freq = 1, algo = 'FedAvg', **kwargs):\n",
        "    self.cnn = model_fn\n",
        "    self.client_list = client_list # calling this param when \"uploading\" or \"downloading\"\n",
        "    self.client_datasize = []\n",
        "    self.generator = generator\n",
        "    self.latent_dim = 128 # hyperparam, can tune\n",
        "    self.public_dataset = None # to generate\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.comm_freq = comm_freq # no. of client local training epochs before upload\n",
        "    self.dataset = None\n",
        "\n",
        "    # default settings for FedAvg\n",
        "    self.is_prune = False\n",
        "\n",
        "    if algo == 'FedPKDG':\n",
        "    # turn on FedPKDG\n",
        "      self.is_prune = True\n",
        "\n",
        "  def get_client_datasize(self):\n",
        "    if len(self.client_datasize) != len(self.client_list):\n",
        "      for i in range(len(self.client_datasize), len(self.client_list)):\n",
        "        self.client_datasize.append(self.client_list[i].get_datasize())\n",
        "    return self.client_datasize\n",
        "\n",
        "  def assign_weights_cnn(self, client):\n",
        "    client.set_cnn_weights(self.cnn.get_weights())\n",
        "\n",
        "  def assign_weights_gen(self, client):\n",
        "    client.set_gen_weights(self.generator.get_weights())\n",
        "\n",
        "  def broadcast(self):\n",
        "    # TODO: improve: can use tff.federated_map and tff.federated_broadcast, can combine the two assign fns\n",
        "    for client in self.client_list:\n",
        "        self.assign_weights_cnn(client)\n",
        "    for client in self.client_list:\n",
        "        self.assign_weights_gen(client)\n",
        "\n",
        "  def local_training(self):\n",
        "    for idx, client in enumerate(self.client_list):\n",
        "      # train cnn only\n",
        "      print('training client', idx, '\\'s cnn')\n",
        "      client.local_train(epochs = self.comm_freq, is_prune = self.is_prune)\n",
        "      # TODO: traingen\n",
        "\n",
        "  # def agg_classifier(self):\n",
        "  #   # !!TODO: can use tff.federated_mean\n",
        "  #   for i in range(len(self.client_list)):\n",
        "  #     # !!TODO: aggregate the classifier layer and freeze it\n",
        "\n",
        "  def agg_cnn(self):\n",
        "    global_weights = [np.zeros_like(w) for w in self.cnn.get_weights()]\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for client_idx, client in enumerate(client_list):\n",
        "        p_k = p[client_idx]/total_size\n",
        "        client_weights = client.get_cnn_weights()\n",
        "        for layer_idx, weight in enumerate(client_weights):\n",
        "            global_weights[layer_idx] += p_k * weight\n",
        "    # Set the updated weights to the global model\n",
        "    self.cnn.set_weights(global_weights)\n",
        "\n",
        "  def agg_gen(self):\n",
        "    global_weights = [np.zeros_like(w) for w in self.generator.get_weights()]\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for client_idx, client in enumerate(client_list):\n",
        "        p_k = p[client_idx]/total_size\n",
        "        client_weights = client.get_gen_weights()\n",
        "        for layer_idx, weight in enumerate(client_weights):\n",
        "            global_weights[layer_idx] += p_k * weight\n",
        "    # Set the updated weights to the global model\n",
        "    self.generator.set_weights(global_weights)\n",
        "\n",
        "  def produce_pseudo_dataset(self, total_num):\n",
        "    # generate with gen, homogenous data: equal number of datapoints for each class\n",
        "    #TODO: test pseudoDataset\n",
        "    self.dataset = pseudoDataset(self.generator, total_num, self.latent_dim)\n",
        "\n",
        "  def agg_logits(self, datapoint):\n",
        "    # mimics clients sending their logits to the server given the same input\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for i in range(len(self.client_list)):\n",
        "      client = self.client_list[i]\n",
        "      p_k = p[i]/total_size\n",
        "      if i == 0:\n",
        "        logits = p_k * client.produce_logits(datapoint)\n",
        "      else:\n",
        "        logits += p_k * client.produce_logits(datapoint)\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def distill(): ##HARD and IMPORTANT!\n",
        "    #!!!TODO: distillation based on self.dataset and agg_logits\n",
        "    return\n"
      ],
      "metadata": {
        "id": "33NE17U-Xr72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Functionality\n",
        "\n",
        "NB: re-run the server block before every experiment to avoid error: Server class not callable"
      ],
      "metadata": {
        "id": "kTbXJ45XOzCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg (Don't Touch)"
      ],
      "metadata": {
        "id": "2jO8wgGy-e62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate 3 clients\n",
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client(smallCNN(), x_train_k, y_train_k))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 1)\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('trained all clients cnn round', _)\n",
        "  Server.agg_cnn()\n",
        "  print('Weighted aggregated client weights')\n",
        "\n",
        "for client in client_list:\n",
        "  client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wx11C-UO0WO",
        "outputId": "af09b480-7040-4a81-be40-cad918d7b828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 6s 14ms/step - loss: 0.8666 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.3297 - val_sparse_categorical_accuracy: 0.9050\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.8512 - sparse_categorical_accuracy: 0.7609 - val_loss: 0.3672 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 7s 19ms/step - loss: 0.8463 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.2613 - val_sparse_categorical_accuracy: 0.9210\n",
            "trained all clients cnn round 0\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 6s 14ms/step - loss: 0.3540 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.2673 - val_sparse_categorical_accuracy: 0.9210\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 6s 12ms/step - loss: 0.3558 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.3160 - val_sparse_categorical_accuracy: 0.9030\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.3469 - sparse_categorical_accuracy: 0.8948 - val_loss: 0.2162 - val_sparse_categorical_accuracy: 0.9395\n",
            "trained all clients cnn round 1\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 5s 12ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.2268 - val_sparse_categorical_accuracy: 0.9405\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 5s 12ms/step - loss: 0.2983 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.2671 - val_sparse_categorical_accuracy: 0.9180\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.1745 - val_sparse_categorical_accuracy: 0.9555\n",
            "trained all clients cnn round 2\n",
            "Weighted aggregated client weights\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2496 - sparse_categorical_accuracy: 0.9256\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9265\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.2532 - sparse_categorical_accuracy: 0.9260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_k = x_train[:no_sample]\n",
        "y_train_k = y_train[:no_sample]\n",
        "client = Client(smallCNN(), x_train_k, y_train_k)\n",
        "client.local_train(epochs=3)\n",
        "client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clXt_GxJD99-",
        "outputId": "9c82cf31-09b6-4fc8-da57-1822796ea9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "282/282 [==============================] - 5s 14ms/step - loss: 0.7805 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.3458 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 2/3\n",
            "282/282 [==============================] - 3s 12ms/step - loss: 0.3653 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.2914 - val_sparse_categorical_accuracy: 0.9185\n",
            "Epoch 3/3\n",
            "282/282 [==============================] - 3s 10ms/step - loss: 0.3179 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.2633 - val_sparse_categorical_accuracy: 0.9245\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2903 - sparse_categorical_accuracy: 0.9135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29031461477279663, 0.9135000109672546]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment: model quickly overfit to bad training data even after syncing weights, need to prevent this"
      ],
      "metadata": {
        "id": "Q5h6K_slalno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just Pruning (Don't Touch)"
      ],
      "metadata": {
        "id": "5nKDiEwT-ayP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client(smallCNN(), x_train_k, y_train_k))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 1, algo = 'FedPKDG')\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('trained all clients cnn round', _)\n",
        "  Server.agg_cnn()\n",
        "  print('Weighted aggregated client weights')\n",
        "\n",
        "for client in client_list:\n",
        "  get_model_sparsity(client.cnn)\n",
        "  client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSmgBFBa-lYt",
        "outputId": "689400d5-ee9b-478b-ee69-0a9220d096f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 9s 13ms/step - loss: 0.8240 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.3441 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 9s 18ms/step - loss: 0.8242 - sparse_categorical_accuracy: 0.7551 - val_loss: 0.4127 - val_sparse_categorical_accuracy: 0.8890\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 12ms/step - loss: 0.8215 - sparse_categorical_accuracy: 0.7543 - val_loss: 0.2855 - val_sparse_categorical_accuracy: 0.9250\n",
            "trained all clients cnn round 0\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 12ms/step - loss: 0.3899 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.3071 - val_sparse_categorical_accuracy: 0.9175\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 14ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.3652 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3844 - sparse_categorical_accuracy: 0.8828 - val_loss: 0.2386 - val_sparse_categorical_accuracy: 0.9335\n",
            "trained all clients cnn round 1\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3507 - sparse_categorical_accuracy: 0.8952 - val_loss: 0.2816 - val_sparse_categorical_accuracy: 0.9190\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 16ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.3383 - val_sparse_categorical_accuracy: 0.9000\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3445 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.2243 - val_sparse_categorical_accuracy: 0.9370\n",
            "trained all clients cnn round 2\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.3172 - sparse_categorical_accuracy: 0.9082\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3139 - sparse_categorical_accuracy: 0.9087\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3113 - sparse_categorical_accuracy: 0.9087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_k = x_train[:no_sample]\n",
        "y_train_k = y_train[:no_sample]\n",
        "client = Client(smallCNN(), x_train_k, y_train_k)\n",
        "client.local_train(epochs = 3, is_prune = True)\n",
        "client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nikIXLJS_NO_",
        "outputId": "62f42ea6-d9f9-4e5f-e359-2cfa16ac0df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin_step= 0 end_step= 844\n",
            "Epoch 1/3\n",
            "282/282 [==============================] - 7s 12ms/step - loss: 0.8385 - sparse_categorical_accuracy: 0.7484 - val_loss: 0.3486 - val_sparse_categorical_accuracy: 0.8985\n",
            "Epoch 2/3\n",
            "282/282 [==============================] - 4s 15ms/step - loss: 0.3786 - sparse_categorical_accuracy: 0.8857 - val_loss: 0.2972 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 3/3\n",
            "282/282 [==============================] - 3s 10ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.2797 - val_sparse_categorical_accuracy: 0.9235\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 0.3094 - sparse_categorical_accuracy: 0.9068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30936574935913086, 0.9067999720573425]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "my58ZMvu-l3U"
      }
    }
  ]
}