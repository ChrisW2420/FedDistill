{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcUGdyKdoMARYf9TiXecUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisW2420/FedPKDG/blob/main/FedPKDG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FedPKDG -- Prune + KD + GAN + FL\n",
        "This prototype implements the algorithm in a distributed setting\n",
        "TODO:\n",
        "1. implement a FedAvg aggregator/server\n",
        "2. build a centralised FL system with n clients connected to the server\n",
        "3. design experiments to assess accuracy, efficiency, generalisation on homogenoeous data\n",
        "4. repeat experiments on heterogeneous data, identical model sparsity\n",
        "5. repeat experiments on heterogeneous data, different model sparsity, mimicing different computational capability of clients"
      ],
      "metadata": {
        "id": "GR5F2i850OT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "0qqRJILKrnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: package versions are very important\n",
        "!pip install -q tensorflow-model-optimization # for pruning\n",
        "!pip install -q git+https://github.com/tensorflow/docs # newest tf\n",
        "!pip install --upgrade keras #newest keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgElPEcBrp3j",
        "outputId": "d7615b33-cb59-4df4-d0f5-50e88a9c68c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.3.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 versions of keras are used for different functionalities, imported as different names\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "import tf_keras as keras_model #only for pruning\n",
        "from tf_keras import layers as model_layers\n",
        "import keras\n",
        "import tempfile\n",
        "from tf_keras.callbacks import EarlyStopping, Callback\n",
        "from keras import ops, layers\n",
        "from tensorflow_docs.vis import embed # for GAN\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0uN9jtQssMKu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging metrics with WandB\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "from wandb.keras import WandbMetricsLogger"
      ],
      "metadata": {
        "id": "yLSfMJe-spr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "lDJsxBBKO5k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST\n",
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "S5AEP2kXO7Zh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components Implementation"
      ],
      "metadata": {
        "id": "UDusEhV8rad4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model zoo"
      ],
      "metadata": {
        "id": "yivLHBBVKZ7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "LiDzieUBHTED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smallCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"smallcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def mediumCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"mediumcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def bigCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"bigcnn\",\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZU1h8xrMKcWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN"
      ],
      "metadata": {
        "id": "2Yeo7R7OHVFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128\n",
        "\n",
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "\n",
        "# Create the discriminator.\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator.\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer((generator_in_channels,)),\n",
        "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "        # 7x7x(128 + num_classes) map.\n",
        "        layers.Dense(7 * 7 * generator_in_channels),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Reshape((7, 7, generator_in_channels)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(negative_slope=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZvYMNrI8HWna"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "zvdZfOosKmTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# very unecessary in this class itself, might be useful to combined pruning and KD and ordinary training\n",
        "class CNN(): #keras_model.Model\n",
        "  def __init__(self, config_name, **kwargs):\n",
        "    #super(CNN, self).__init__(**kwargs)\n",
        "    if config_name == 'small':\n",
        "      self.model = smallCNN()\n",
        "    elif config_name == 'medium':\n",
        "      self.model = mediumCNN()\n",
        "    elif config_name == 'big':\n",
        "      self.model = bigCNN()\n",
        "    else:\n",
        "      print('default model of medium CNN')\n",
        "      self.model = mediumCNN()\n",
        "    self.optimizer = 'adam'\n",
        "    self.task_loss = keras_model.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.metricslist = [keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "    self.validation_split = 0.1\n",
        "    self.early_stopping = EarlyStopping(\n",
        "      monitor='val_loss',\n",
        "      min_delta=0.001,  # only consider as improvement significant changes\n",
        "      patience=2,      # number of epochs with no improvement after which training will be stopped\n",
        "      verbose=1,\n",
        "      mode='min'        # 'min' because we want to minimize the loss\n",
        "    )\n",
        "    self.callbacks = []\n",
        "\n",
        "\n",
        "  def train(self, training_data, testing_data = None, epochs = 10, is_earlystop = True, **kwargs):\n",
        "    #super(CNN, self).compile(optimizer=self.optimizer, metrics=self.metricslist, **kwargs)\n",
        "    self.model.compile(\n",
        "      optimizer=self.optimizer,\n",
        "      loss=self.task_loss,\n",
        "      metrics=self.metricslist\n",
        "    )\n",
        "    if is_earlystop and self.early_stopping not in self.callbacks:\n",
        "      self.callbacks.append(self.early_stopping)\n",
        "    x_train, y_train = training_data\n",
        "    self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,validation_split=self.validation_split, callbacks=self.callbacks)\n",
        "    if testing_data:\n",
        "      x_test, y_test = testing_data\n",
        "      self.model.evaluate(x_test, y_test)\n",
        "    return self.model\n"
      ],
      "metadata": {
        "id": "buLK7EZXKkBL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN"
      ],
      "metadata": {
        "id": "C9du692bJ1GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = ops.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = ops.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = ops.shape(real_images)[0]\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = ops.concatenate(\n",
        "            [generated_images, image_one_hot_labels], -1\n",
        "        )\n",
        "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = ops.concatenate(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = ops.concatenate(\n",
        "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = ops.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = ops.concatenate(\n",
        "                [fake_images, image_one_hot_labels], -1\n",
        "            )\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "dimSAKxdJ9jq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image generation functions\n",
        "def generate_image(generator, target_class, latent_dim):\n",
        "    noise_matrix = keras.random.normal(shape=(1, latent_dim))\n",
        "    # Convert the target label to one-hot encoded vectors.\n",
        "    target_label = keras.utils.to_categorical([target_class], num_classes)\n",
        "    target_label = ops.cast(target_label, \"float32\")\n",
        "    noise_and_labels = ops.concatenate([noise_matrix, target_label], 1)\n",
        "    fake = generator.predict(noise_and_labels,verbose = 0)\n",
        "    return fake\n",
        "\n",
        "def pseudoDataset(generator, total_num, latent_dim): # producing equal numbers of samples for each class\n",
        "    pseudo_images = []\n",
        "    for num in range(10):\n",
        "      target_class = num\n",
        "      print('Generating', int(total_num/10), 'fake images of digit', num, '......')\n",
        "      for _ in range(int(total_num/10)):\n",
        "        generated_images = generate_image(target_class, latent_dim)\n",
        "        generated_images *= 255.0\n",
        "        converted_images = generated_images.astype(np.uint8)\n",
        "        converted_images = ops.image.resize(converted_images, (28, 28)).numpy().astype(np.uint8)\n",
        "        pseudo_images.append(converted_images)\n",
        "    pseudo_images = np.concatenate(pseudo_images, axis=0)\n",
        "    pseudo_labels = np.repeat(np.arange(10), int(total_num/10))\n",
        "    return pseudo_images, pseudo_labels"
      ],
      "metadata": {
        "id": "IYYsK1c3nEry"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning"
      ],
      "metadata": {
        "id": "EZI_-kzpJ4OC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oxgClZVAJ-Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation"
      ],
      "metadata": {
        "id": "m0uflogyJ53G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNTcgLrs0PmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function Implementation\n",
        "\n",
        "TODO:\n",
        "Dataset:\n",
        "- Dataloader\n",
        "- heterogeneous dataset partition\n",
        "- data augmentation\n",
        "\n",
        "visualisation:\n",
        "- dataset example visualisation\n",
        "- data distribution visualisation\n",
        "- confusion matrix\n",
        "-"
      ],
      "metadata": {
        "id": "rtt0HHPp6bWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_model_weights_to_zero(model):\n",
        "    for layer in model.layers:\n",
        "        zero_weights = [np.zeros_like(w) for w in layer.get_weights()]\n",
        "        layer.set_weights(zero_weights)\n",
        "    return model"
      ],
      "metadata": {
        "id": "pL7iWz7Z6ew6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback zoo"
      ],
      "metadata": {
        "id": "0_Ji1-THrXgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,  # only consider as improvement significant changes\n",
        "    patience=2,      # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min'        # 'min' because we want to minimize the loss\n",
        "    )"
      ],
      "metadata": {
        "id": "9di4sjQurWkv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client Implementation"
      ],
      "metadata": {
        "id": "0XwMW_rz5C_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Client():\n",
        "  def __init__(self, model_config_name, train_dataset, generator = generator, discriminator = discriminator):\n",
        "    self.cnn = smallCNN() #TODO: change this to taking in different models\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.latent_dim = 128 # hyperparam, can tune\n",
        "    self.private_dataset = train_dataset\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.validation_split=0.1\n",
        "\n",
        "  def local_train(self, epochs = 5, is_prune = False, sparsity = 0.5, fine_tune_epochs = 2):\n",
        "    if is_prune:\n",
        "      #!!!TODO: prune\n",
        "      return self.cnn\n",
        "    #TODO: test train_CNN\n",
        "    self.train_cnn(epochs)\n",
        "    return self.cnn\n",
        "\n",
        "  def train_cnn(self, epochs):\n",
        "    self.cnn.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "    self.cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,validation_split=self.validation_split, callbacks=[early_stopping])\n",
        "\n",
        "  # ! TODO: cnn evaluation def eval_cnn\n",
        "\n",
        "  def train_gen(self, epochs = 20, d_learning_rate = 0.0003, g_learning_rate = 0.0003):\n",
        "    #TODO: test ConditionalGAN\n",
        "    cond_gan = ConditionalGAN(self.discriminator, self.generator, self.latent_dim)\n",
        "    cond_gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(d_learning_rate),\n",
        "        g_optimizer=keras.optimizers.Adam(g_learning_rate),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    )\n",
        "    # produce GAN training dataset\n",
        "    train_label = keras.utils.to_categorical(self.y_train, 10) # 1 hot encoding label\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((self.x_train, train_label))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    cond_gan.fit(dataset, epochs)\n",
        "\n",
        "\n",
        "  # interface with the server\n",
        "  def get_generator(self):\n",
        "    return self.generator\n",
        "\n",
        "  def produce_logits(self, dataset): #for KD\n",
        "    #!!!TODO\n",
        "    logits = []\n",
        "    return logits\n",
        "\n",
        "  def get_cnn(self): #only for FedAvg, disabled\n",
        "    return self.cnn"
      ],
      "metadata": {
        "id": "oYQ-XPX05CW2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server Implementation"
      ],
      "metadata": {
        "id": "8RMvpBJwLuju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Server():\n",
        "  def __init__(self, model_fn, client_list, generator = generator, comm_freq = 5):\n",
        "    self.cnn = model_fn\n",
        "    self.client_list = client_list # calling this param when \"uploading\" or \"downloading\"\n",
        "    self.client_datasize = []\n",
        "    self.generator = generator\n",
        "    self.latent_dim = 128 # hyperparam, can tune\n",
        "    self.public_dataset = None # to generate\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.comm_freq = comm_freq # no. of client local training epochs before upload\n",
        "    self.dataset = None\n",
        "\n",
        "  def get_client_datasize(self):\n",
        "    if len(self.client_datasize) != len(self.client_list):\n",
        "      for i in range(len(self.client_datasize), len(self.client_list)):\n",
        "        self.client_datasize.append(self.client_list[i].private_dataset[0].shape[0])\n",
        "    return self.client_datasize\n",
        "\n",
        "  def assign_weights_cnn(self, client):\n",
        "    client.cnn.set_weights(self.cnn.get_weights())\n",
        "\n",
        "  def assign_weights_gen(self, client):\n",
        "    client.generator.set_weights(self.generator.get_weights())\n",
        "\n",
        "  def broadcast(self):\n",
        "    # TODO: improve: can use tff.federated_map and tff.federated_broadcast, can combine the two assign fns\n",
        "    map(self.assign_weights_cnn, self.client_list)\n",
        "    map(self.assign_weights_gen, self.client_list)\n",
        "\n",
        "  def local_training(self):\n",
        "    for idx, client in enumerate(self.client_list):\n",
        "      # train cnn only\n",
        "      print('training client', idx, '\\'s cnn')\n",
        "      client.local_train(epochs = self.comm_freq)\n",
        "      # TODO: traingen\n",
        "\n",
        "  # def agg_classifier(self):\n",
        "  #   # !!TODO: can use tff.federated_mean\n",
        "  #   for i in range(len(self.client_list)):\n",
        "  #     # !!TODO: aggregate the classifier layer and freeze it\n",
        "\n",
        "  def agg_cnn(self): #for FedAvg only\n",
        "    self.cnn = set_model_weights_to_zero(self.cnn)\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for i in range(len(self.client_list)):\n",
        "      p_k = p[i]/total_size\n",
        "      for global_weights, client_weights in zip(self.cnn.get_weights(), self.client_list[i].cnn.get_weights()):\n",
        "        global_weights = global_weights + p_k* client_weights\n",
        "\n",
        "  def agg_gen(self):\n",
        "    # same can be implemented with tff\n",
        "    self.generator = set_model_weights_to_zero(self.generator)\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for i in range(len(self.client_list)):\n",
        "      p_k = p[i]/total_size\n",
        "      for global_weights, client_weights in zip(self.generator.get_weights(), self.client_list[i].generator.get_weights()):\n",
        "        global_weights = global_weights + p_k* client_weights\n",
        "\n",
        "  def produce_pseudo_dataset(self, total_num):\n",
        "    # generate with gen, homogenous data: equal number of datapoints for each class\n",
        "    #TODO: test pseudoDataset\n",
        "    self.dataset = pseudoDataset(self.generator, total_num, self.latent_dim)\n",
        "\n",
        "  def agg_logits(self, datapoint):\n",
        "    # mimics clients sending their logits to the server given the same input\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for i in range(len(self.client_list)):\n",
        "      client = self.client_list[i]\n",
        "      p_k = p[i]/total_size\n",
        "      if i == 0:\n",
        "        logits = p_k * client.produce_logits(datapoint)\n",
        "      else:\n",
        "        logits += p_k * client.produce_logits(datapoint)\n",
        "    return logits\n",
        "\n",
        "\n",
        "  def distill(): ##HARD and IMPORTANT!\n",
        "    #!!!TODO: distillation based on self.dataset and agg_logits\n",
        "    return\n"
      ],
      "metadata": {
        "id": "33NE17U-Xr72"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing - FedAvg"
      ],
      "metadata": {
        "id": "kTbXJ45XOzCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate 5 clients\n",
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client('small', (x_train_k, y_train_k)))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 1)\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('trained all clients cnn round', _)\n",
        "  Server.agg_cnn()\n",
        "  print('Weighted aggregated client weights')\n",
        "\n",
        "for client in client_list:\n",
        "  client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wx11C-UO0WO",
        "outputId": "d7b80759-83a8-48c6-dfd5-efc87f8fc4ea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "844/844 [==============================] - 10s 11ms/step - loss: 0.4781 - sparse_categorical_accuracy: 0.8641 - val_loss: 0.2011 - val_sparse_categorical_accuracy: 0.9437\n",
            "training client 1 's cnn\n",
            "844/844 [==============================] - 14s 14ms/step - loss: 0.4530 - sparse_categorical_accuracy: 0.8640 - val_loss: 0.1593 - val_sparse_categorical_accuracy: 0.9542\n",
            "training client 2 's cnn\n",
            "844/844 [==============================] - 11s 11ms/step - loss: 0.5105 - sparse_categorical_accuracy: 0.8466 - val_loss: 0.2209 - val_sparse_categorical_accuracy: 0.9402\n",
            "trained all clients cnn round 0\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "844/844 [==============================] - 11s 12ms/step - loss: 0.1774 - sparse_categorical_accuracy: 0.9478 - val_loss: 0.1079 - val_sparse_categorical_accuracy: 0.9690\n",
            "training client 1 's cnn\n",
            "844/844 [==============================] - 11s 12ms/step - loss: 0.1665 - sparse_categorical_accuracy: 0.9497 - val_loss: 0.1226 - val_sparse_categorical_accuracy: 0.9663\n",
            "training client 2 's cnn\n",
            "844/844 [==============================] - 12s 13ms/step - loss: 0.1927 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.1152 - val_sparse_categorical_accuracy: 0.9672\n",
            "trained all clients cnn round 1\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "844/844 [==============================] - 10s 10ms/step - loss: 0.1220 - sparse_categorical_accuracy: 0.9635 - val_loss: 0.0894 - val_sparse_categorical_accuracy: 0.9740\n",
            "training client 1 's cnn\n",
            "844/844 [==============================] - 10s 10ms/step - loss: 0.1373 - sparse_categorical_accuracy: 0.9584 - val_loss: 0.1078 - val_sparse_categorical_accuracy: 0.9688\n",
            "training client 2 's cnn\n",
            "844/844 [==============================] - 10s 10ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.0979 - val_sparse_categorical_accuracy: 0.9730\n",
            "trained all clients cnn round 2\n",
            "Weighted aggregated client weights\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 0.0944 - sparse_categorical_accuracy: 0.9691\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9653\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0976 - sparse_categorical_accuracy: 0.9693\n"
          ]
        }
      ]
    }
  ]
}