{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPddIvCQeTQXH++F/cfFvN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisW2420/FedPKDG/blob/main/FedPKDG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FedPKDG -- Prune + KD + GAN + FL\n",
        "This prototype implements the algorithm in a distributed setting\n",
        "TODO:\n",
        "1. implement a FedAvg aggregator/server\n",
        "2. build a centralised FL system with n clients connected to the server\n",
        "3. design experiments to assess accuracy, efficiency, generalisation on homogenoeous data\n",
        "4. repeat experiments on heterogeneous data, identical model sparsity\n",
        "5. repeat experiments on heterogeneous data, different model sparsity, mimicing different computational capability of clients"
      ],
      "metadata": {
        "id": "GR5F2i850OT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "0qqRJILKrnul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: package versions are very important\n",
        "!pip install -q tensorflow-model-optimization # for pruning\n",
        "!pip install -q git+https://github.com/tensorflow/docs # newest tf\n",
        "!pip install --upgrade keras #newest keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgElPEcBrp3j",
        "outputId": "90210e0e-4165-4c85-da96-bdaffd41b221"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/242.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/242.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/242.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Collecting namex (from keras)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\n",
            "Collecting optree (from keras)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Installing collected packages: namex, optree, keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.3.3 namex-0.0.8 optree-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 versions of keras are used for different functionalities, imported as different names\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "import tf_keras as keras_model #only for pruning\n",
        "from tf_keras import layers as model_layers\n",
        "import keras\n",
        "import tempfile\n",
        "from tf_keras.callbacks import EarlyStopping, Callback\n",
        "from keras import ops, layers\n",
        "from tensorflow_docs.vis import embed # for GAN\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "0uN9jtQssMKu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging metrics with WandB\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()\n",
        "from wandb.keras import WandbMetricsLogger"
      ],
      "metadata": {
        "id": "yLSfMJe-spr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "lDJsxBBKO5k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST\n",
        "# Prepare the train and test dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
      ],
      "metadata": {
        "id": "S5AEP2kXO7Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e027e08-254e-43df-d166-5889fd013dca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components Implementation"
      ],
      "metadata": {
        "id": "UDusEhV8rad4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model zoo"
      ],
      "metadata": {
        "id": "yivLHBBVKZ7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "LiDzieUBHTED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def miniCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"minicnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def smallCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"smallcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def mediumCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(8, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"mediumcnn\",\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def bigCNN():\n",
        "  model = keras_model.Sequential(\n",
        "      [\n",
        "          keras_model.Input(shape=(28, 28, 1)),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.LeakyReLU(alpha=0.2),\n",
        "          model_layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
        "          model_layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          model_layers.Flatten(),\n",
        "          model_layers.Dense(10),\n",
        "      ],\n",
        "      name=\"bigcnn\",\n",
        "  )\n",
        "  return model"
      ],
      "metadata": {
        "id": "ZU1h8xrMKcWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN"
      ],
      "metadata": {
        "id": "2Yeo7R7OHVFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128 # hyperparam, can tune\n",
        "\n",
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes\n",
        "\n",
        "# Create the discriminator.\n",
        "def Discriminator(latent_dim = 128):\n",
        "  discriminator = keras.Sequential(\n",
        "      [\n",
        "          keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "          layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          layers.LeakyReLU(negative_slope=0.2),\n",
        "          layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "          layers.LeakyReLU(negative_slope=0.2),\n",
        "          layers.GlobalMaxPooling2D(),\n",
        "          layers.Dense(1),\n",
        "      ],\n",
        "      name=\"discriminator\",\n",
        "  )\n",
        "  return discriminator\n",
        "\n",
        "# Create the generator.\n",
        "def Generator():\n",
        "  generator = keras.Sequential(\n",
        "      [\n",
        "          keras.layers.InputLayer((generator_in_channels,)),\n",
        "          # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "          # 7x7x(128 + num_classes) map.\n",
        "          layers.Dense(7 * 7 * generator_in_channels),\n",
        "          layers.LeakyReLU(negative_slope=0.2),\n",
        "          layers.Reshape((7, 7, generator_in_channels)),\n",
        "          layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "          layers.LeakyReLU(negative_slope=0.2),\n",
        "          layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "          layers.LeakyReLU(negative_slope=0.2),\n",
        "          layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "      ],\n",
        "      name=\"generator\",\n",
        "  )\n",
        "  return generator"
      ],
      "metadata": {
        "id": "ZvYMNrI8HWna"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN"
      ],
      "metadata": {
        "id": "C9du692bJ1GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = ops.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = ops.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = ops.shape(real_images)[0]\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = ops.concatenate(\n",
        "            [generated_images, image_one_hot_labels], -1\n",
        "        )\n",
        "        real_image_and_labels = ops.concatenate([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = ops.concatenate(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = ops.concatenate(\n",
        "            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = keras.random.normal(\n",
        "            shape=(batch_size, self.latent_dim), seed=self.seed_generator\n",
        "        )\n",
        "        random_vector_labels = ops.concatenate(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = ops.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = ops.concatenate(\n",
        "                [fake_images, image_one_hot_labels], -1\n",
        "            )\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "dimSAKxdJ9jq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image generation functions\n",
        "def generate_image(generator, target_class, latent_dim):\n",
        "    noise_matrix = keras.random.normal(shape=(1, latent_dim))\n",
        "    # Convert the target label to one-hot encoded vectors.\n",
        "    target_label = keras.utils.to_categorical([target_class], num_classes)\n",
        "    target_label = ops.cast(target_label, \"float32\")\n",
        "    noise_and_labels = ops.concatenate([noise_matrix, target_label], 1)\n",
        "    fake = generator.predict(noise_and_labels,verbose = 0)\n",
        "    return fake\n",
        "\n",
        "def pseudoDataset(generator, total_num, latent_dim): # producing equal numbers of samples for each class\n",
        "    pseudo_images = []\n",
        "    for num in range(10):\n",
        "      target_class = num\n",
        "      print('Generating', int(total_num/10), 'fake images of digit', num, '......')\n",
        "      for _ in range(int(total_num/10)):\n",
        "        generated_images = generate_image(generator, target_class, latent_dim)\n",
        "        generated_images *= 255.0\n",
        "        converted_images = generated_images.astype(np.uint8)\n",
        "        converted_images = ops.image.resize(converted_images, (28, 28)).numpy().astype(np.uint8)\n",
        "        pseudo_images.append(converted_images)\n",
        "    pseudo_images = np.concatenate(pseudo_images, axis=0)\n",
        "    pseudo_labels = np.repeat(np.arange(10), int(total_num/10))\n",
        "    return pseudo_images, pseudo_labels"
      ],
      "metadata": {
        "id": "IYYsK1c3nEry"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning"
      ],
      "metadata": {
        "id": "EZI_-kzpJ4OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_finetrain(base_model, _epochs, x, y, target_sparsity, fine_tune_epochs, validation_split=0.1):\n",
        "  callbacks = [\n",
        "      sparsity.UpdatePruningStep(),\n",
        "      early_stopping\n",
        "  ]\n",
        "  steps_per_epoch = len(x)*(1-validation_split) // batch_size\n",
        "  begin_step=int(steps_per_epoch*fine_tune_epochs)\n",
        "  end_step=int(steps_per_epoch*_epochs)+1\n",
        "  print('begin_step=', begin_step, 'end_step=', end_step)\n",
        "  pruning_schedule = sparsity.PolynomialDecay(initial_sparsity=0, final_sparsity=target_sparsity,\n",
        "                                              begin_step=begin_step, end_step=end_step) # TODO: tune begin_step, consider fining training before starting to prune\n",
        "\n",
        "  model_for_pruning = sparsity.prune_low_magnitude(base_model, pruning_schedule=pruning_schedule) #default constant sparsity of 50%\n",
        "\n",
        "  model_for_pruning.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "  )\n",
        "\n",
        "  model_for_pruning.fit(\n",
        "      x,\n",
        "      y,\n",
        "      batch_size=batch_size,\n",
        "      validation_split=validation_split,\n",
        "      callbacks=callbacks,\n",
        "      epochs=_epochs,\n",
        "  )\n",
        "  pruned_model = sparsity.strip_pruning(model_for_pruning)\n",
        "\n",
        "  return pruned_model\n",
        "\n",
        "\n",
        "# Model size metrics\n",
        "\n",
        "def get_model_sparsity(model):\n",
        "    total_weights = 0\n",
        "    zero_weights = 0\n",
        "    for weight in model.get_weights():\n",
        "        total_weights += weight.size\n",
        "        zero_weights += np.count_nonzero(weight == 0)\n",
        "    return zero_weights / total_weights\n",
        "\n",
        "def get_gzipped_model_size(model):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  _, keras_file = tempfile.mkstemp('.h5')\n",
        "  model.save(keras_file, include_optimizer=False)\n",
        "\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(keras_file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "oxgClZVAJ-Y2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Distillation"
      ],
      "metadata": {
        "id": "m0uflogyJ53G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distiller(keras_model.Model):\n",
        "    def __init__(self, get_teacher_logits, student, alpha=0.1, temperature=3, **kwargs):\n",
        "        super(Distiller, self).__init__(**kwargs)\n",
        "        self.student = student\n",
        "        self.get_teacher_logits = get_teacher_logits\n",
        "\n",
        "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha, temperature, **kwargs):\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics, **kwargs)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.student.compile(optimizer=optimizer, metrics=metrics, loss=self.student_loss_fn)\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        teacher_predictions = self.get_teacher_logits(x)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of the student\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Calculate the task-specific loss\n",
        "            task_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # Calculate the soft targets and the distillation loss\n",
        "            soft_targets = tf.nn.softmax(teacher_predictions / self.temperature)\n",
        "\n",
        "            student_soft = tf.nn.softmax(student_predictions / self.temperature)\n",
        "            distillation_loss = self.distillation_loss_fn(soft_targets, student_soft)\n",
        "\n",
        "            # Calculate the total loss\n",
        "            total_loss = (1 - self.alpha) * task_loss + self.alpha * distillation_loss * (self.temperature ** 2)\n",
        "\n",
        "        # Compute gradients and update weights\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(total_loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"task_loss\": task_loss, \"distillation_loss\": distillation_loss, \"total_loss\": total_loss})\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of the student\n",
        "        y_pred = self.student(x, training=False)\n",
        "\n",
        "        # Calculate the task-specific loss\n",
        "        task_loss = self.student_loss_fn(y, y_pred)\n",
        "\n",
        "        # Update the metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call_model(self):\n",
        "      return self.student"
      ],
      "metadata": {
        "id": "BNTcgLrs0PmV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Function Implementation\n",
        "\n",
        "TODO:\n",
        "Dataset:\n",
        "- Dataloader\n",
        "- heterogeneous dataset partition\n",
        "- data augmentation\n",
        "\n",
        "visualisation:\n",
        "- dataset example visualisation\n",
        "- data distribution visualisation\n",
        "- confusion matrix\n",
        "-"
      ],
      "metadata": {
        "id": "rtt0HHPp6bWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_model_weights_to_zero(model):\n",
        "    for layer in model.layers:\n",
        "        zero_weights = [np.zeros_like(w) for w in layer.get_weights()]\n",
        "        layer.set_weights(zero_weights)\n",
        "    return model"
      ],
      "metadata": {
        "id": "pL7iWz7Z6ew6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def if_synced(model1, model2):\n",
        "    for layer1, layer2 in zip(model1.layers, model2.layers):\n",
        "          weights1 = layer1.get_weights()\n",
        "          weights2 = layer2.get_weights()\n",
        "          for w1, w2 in zip(weights1, weights2):\n",
        "              if not np.array_equal(w1, w2):\n",
        "                  print('different weights, syncing failed')\n",
        "    print('weights synced for client')"
      ],
      "metadata": {
        "id": "5nd729kbESwD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback zoo"
      ],
      "metadata": {
        "id": "0_Ji1-THrXgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0.001,  # only consider as improvement significant changes\n",
        "    patience=2,      # number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='min'        # 'min' because we want to minimize the loss\n",
        "    )"
      ],
      "metadata": {
        "id": "9di4sjQurWkv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Client"
      ],
      "metadata": {
        "id": "0XwMW_rz5C_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Client(): #TODO: add name to clients to refer to them, espeically during logging\n",
        "  def __init__(self, model_fn, x_train, y_train, **kwargs): #generator = None, discriminator = None, self.latent_dim = 128\n",
        "    self.cnn = model_fn\n",
        "    self.generator = Generator()\n",
        "    self.discriminator = Discriminator()\n",
        "    self.latent_dim = 128\n",
        "    self.x_private = x_train\n",
        "    self.y_private = y_train\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.validation_split=0.1\n",
        "\n",
        "  def local_train(self, epochs = 5, is_prune = False, sparsity = 0.4, fine_tune_epochs = 0, **kwargs):\n",
        "    if is_prune:\n",
        "      print('from gloabl - before pruning client has sparsity', get_model_sparsity(self.cnn))\n",
        "      self.cnn = prune_finetrain(self.cnn, _epochs = epochs, x = self.x_private, y = self.y_private, target_sparsity = sparsity, fine_tune_epochs = fine_tune_epochs) # fine_tune_epochs can take decimals, starts pruning after fine tune\n",
        "      self.cnn.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "      print('after pruning client has sparsity', get_model_sparsity(self.cnn))\n",
        "    else:\n",
        "      self.cnn.compile(\n",
        "        optimizer='adam',\n",
        "        loss=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras_model.metrics.SparseCategoricalAccuracy()]\n",
        "      )\n",
        "      self.cnn.fit(self.x_private, self.y_private, batch_size=batch_size, epochs=epochs,validation_split=self.validation_split, callbacks=[early_stopping])\n",
        "\n",
        "  def train_gen(self, epochs = 20, d_learning_rate = 0.0003, g_learning_rate = 0.0003):\n",
        "    #TODO: test ConditionalGAN\n",
        "    cond_gan = ConditionalGAN(self.discriminator, self.generator, self.latent_dim)\n",
        "    cond_gan.compile(\n",
        "        d_optimizer=keras.optimizers.Adam(d_learning_rate),\n",
        "        g_optimizer=keras.optimizers.Adam(g_learning_rate),\n",
        "        loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    )\n",
        "    # produce GAN training dataset\n",
        "    train_label = keras.utils.to_categorical(self.y_private, 10) # 1 hot encoding label\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((self.x_private, train_label))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "    cond_gan.fit(dataset, epochs=epochs)\n",
        "\n",
        "\n",
        "  ## the following code act as interface with the server, avoid direct access to private model and dataset\n",
        "\n",
        "  def produce_logits(self, x): #for KD\n",
        "    logits = self.cnn(x, training=False)\n",
        "    print('----getting 1 client logits')\n",
        "    return logits\n",
        "\n",
        "  def get_cnn_weights(self): #only for FedAvg, disabled\n",
        "    return self.cnn.get_weights()\n",
        "\n",
        "  def get_cnn_classifier(self):\n",
        "    return self.cnn.layers[-1].get_weights()\n",
        "\n",
        "  def set_cnn_weights(self, weights): #for downloading global weights\n",
        "    self.cnn.set_weights(weights)\n",
        "\n",
        "  def get_gen_weights(self): #only for FedAvg, disabled\n",
        "    return self.generator.get_weights()\n",
        "\n",
        "  def set_gen_weights(self, weights): #for downloading global weights\n",
        "    self.generator.set_weights(weights)\n",
        "\n",
        "  def get_datasize(self):\n",
        "    return self.x_private.shape[0]"
      ],
      "metadata": {
        "id": "oYQ-XPX05CW2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Server"
      ],
      "metadata": {
        "id": "8RMvpBJwLuju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Server():\n",
        "  def __init__(self, model_fn, client_list, comm_freq = 1, algo = 'FedAvg', **kwargs): #generator = None,\n",
        "    self.cnn = model_fn\n",
        "    self.client_list = client_list # calling this param when \"uploading\" or \"downloading\"\n",
        "    self.client_datasize = []\n",
        "    self.generator = Generator()\n",
        "    self.latent_dim = 128 # hyperparam, can tune\n",
        "    self.public_dataset = None # to generate\n",
        "    self.batch_size = 64 # hyperparam, can tune\n",
        "    self.comm_freq = comm_freq # no. of client local training epochs before upload\n",
        "\n",
        "    # default settings for FedAvg\n",
        "    self.is_prune = False\n",
        "    self.is_simKD = False\n",
        "\n",
        "    if algo == 'FedPKDG':\n",
        "    # turn on FedPKDG\n",
        "      self.is_prune = True\n",
        "      self.is_simKD = True\n",
        "\n",
        "  def get_client_datasize(self):\n",
        "    if len(self.client_datasize) != len(self.client_list):\n",
        "      for i in range(len(self.client_datasize), len(self.client_list)):\n",
        "        self.client_datasize.append(self.client_list[i].get_datasize())\n",
        "    return self.client_datasize\n",
        "\n",
        "  def assign_weights_cnn(self, client):\n",
        "    client.set_cnn_weights(self.cnn.get_weights())\n",
        "\n",
        "  def assign_weights_gen(self, client):\n",
        "    client.set_gen_weights(self.generator.get_weights())\n",
        "\n",
        "  def broadcast(self):\n",
        "    # TODO: improve: can use tff.federated_map and tff.federated_broadcast, can combine the two assign fns\n",
        "    for client in self.client_list:\n",
        "        self.assign_weights_cnn(client)\n",
        "    for client in self.client_list:\n",
        "        self.assign_weights_gen(client)\n",
        "\n",
        "  def local_training(self):\n",
        "    for idx, client in enumerate(self.client_list):\n",
        "      # train the cnn\n",
        "      print('training client', idx, '\\'s CNN')\n",
        "      client.local_train(epochs = self.comm_freq, is_prune = self.is_prune)\n",
        "      # train the generator\n",
        "      print('training client', idx, '\\'s GEN')\n",
        "      client.train_gen(epochs = self.comm_freq)\n",
        "\n",
        "  def weighted_average(self, type_of_value ,output):\n",
        "    print('----getting weighted avg of clients\\'', type_of_value)\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for client_idx, client in enumerate(self.client_list):\n",
        "        p_k = p[client_idx]/total_size\n",
        "        if type_of_value == 'cnn':\n",
        "          client_val = client.get_cnn_weights()\n",
        "        elif type_of_value == 'gen':\n",
        "          client_val = client.get_gen_weights()\n",
        "        elif type_of_value == 'classifier':\n",
        "          client_val = client.get_cnn_classifier()\n",
        "        for val_idx, value in enumerate(client_val):\n",
        "          output[val_idx] += p_k * value\n",
        "    return output\n",
        "\n",
        "  def agg_cnn(self):\n",
        "    global_weights = [np.zeros_like(w) for w in self.cnn.get_weights()]\n",
        "    global_weights = self.weighted_average('cnn', global_weights)\n",
        "    # Set the updated weights to the global model\n",
        "    self.cnn.set_weights(global_weights)\n",
        "\n",
        "  def agg_classifier(self):\n",
        "    # !!TODO: to test, can use tff.federated_mean\n",
        "    global_weights = [np.zeros_like(w) for w in self.cnn.layers[-1].get_weights()]\n",
        "    global_weights = self.weighted_average('classifier', global_weights)\n",
        "    # Set the updated weights to the global model\n",
        "    self.cnn.layers[-1].set_weights(global_weights)\n",
        "    self.cnn.layers[-1].trainable = False\n",
        "\n",
        "  def agg_gen(self):\n",
        "    global_weights = [np.zeros_like(w) for w in self.generator.get_weights()]\n",
        "    global_weights = self.weighted_average('gen', global_weights)\n",
        "    # Set the updated weights to the global model\n",
        "    self.generator.set_weights(global_weights)\n",
        "\n",
        "  def produce_pseudo_dataset(self, total_num = None):\n",
        "    # generate with gen, homogenous data: equal number of datapoints for each class\n",
        "    #TODO: test pseudoDataset\n",
        "    if total_num == None:\n",
        "      total_num = 100 #max(self.client_datasize)\n",
        "    if self.public_dataset == None:\n",
        "      self.public_dataset = pseudoDataset(generator=self.generator, total_num=total_num, latent_dim=self.latent_dim)\n",
        "    else:\n",
        "      self.public_dataset.append(pseudoDataset(generator=self.generator, total_num=total_num, latent_dim=self.latent_dim))\n",
        "\n",
        "  def agg_logits(self, data):\n",
        "    # mimics clients sending their logits to the server given the same input\n",
        "    p = self.get_client_datasize()\n",
        "    total_size = sum(p)\n",
        "    for client_idx, client in enumerate(self.client_list):\n",
        "      p_k = p[client_idx]/total_size\n",
        "      if client_idx == 0:\n",
        "        logits = p_k * client.produce_logits(data)\n",
        "      else:\n",
        "        logits += p_k * client.produce_logits(data)\n",
        "    print('----getting aggregate client logits',logits)\n",
        "    return logits\n",
        "\n",
        "  def distill_to_global(self, epochs=3):\n",
        "    #!!!TODO: test distillation based on self.public_dataset and agg_logits\n",
        "    if self.is_simKD:\n",
        "      self.agg_classifier()\n",
        "    distiller = Distiller(get_teacher_logits = self.agg_logits, student = self.cnn)\n",
        "    distiller.compile(\n",
        "      optimizer=keras_model.optimizers.Adam(),\n",
        "      metrics=[keras_model.metrics.SparseCategoricalAccuracy()],\n",
        "      student_loss_fn=keras_model.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      distillation_loss_fn=keras_model.losses.KLDivergence(),\n",
        "      alpha=0.4,\n",
        "      temperature=3,\n",
        "    )\n",
        "    # Distill teacher to student\n",
        "    distiller.fit(self.public_dataset, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "    print('should print not trainable:', self.cnn.layers[-1].trainable)\n",
        "    self.cnn.layers[-1].trainable = True\n",
        "    print('should print trainable:', self.cnn.layers[-1].trainable)"
      ],
      "metadata": {
        "id": "33NE17U-Xr72"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Functionality\n",
        "\n",
        "NB: re-run the server block before every experiment to avoid error: Server class not callable"
      ],
      "metadata": {
        "id": "kTbXJ45XOzCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg (Don't Touch)"
      ],
      "metadata": {
        "id": "2jO8wgGy-e62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate 3 clients\n",
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client(smallCNN(), x_train_k, y_train_k))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 1)\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('trained all clients cnn round', _)\n",
        "  Server.agg_cnn()\n",
        "  print('Weighted aggregated client weights')\n",
        "\n",
        "for client in client_list:\n",
        "  client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wx11C-UO0WO",
        "outputId": "af09b480-7040-4a81-be40-cad918d7b828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 6s 14ms/step - loss: 0.8666 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.3297 - val_sparse_categorical_accuracy: 0.9050\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.8512 - sparse_categorical_accuracy: 0.7609 - val_loss: 0.3672 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 7s 19ms/step - loss: 0.8463 - sparse_categorical_accuracy: 0.7662 - val_loss: 0.2613 - val_sparse_categorical_accuracy: 0.9210\n",
            "trained all clients cnn round 0\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 6s 14ms/step - loss: 0.3540 - sparse_categorical_accuracy: 0.8938 - val_loss: 0.2673 - val_sparse_categorical_accuracy: 0.9210\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 6s 12ms/step - loss: 0.3558 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.3160 - val_sparse_categorical_accuracy: 0.9030\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.3469 - sparse_categorical_accuracy: 0.8948 - val_loss: 0.2162 - val_sparse_categorical_accuracy: 0.9395\n",
            "trained all clients cnn round 1\n",
            "Weighted aggregated client weights\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "282/282 [==============================] - 5s 12ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.2268 - val_sparse_categorical_accuracy: 0.9405\n",
            "training client 1 's cnn\n",
            "282/282 [==============================] - 5s 12ms/step - loss: 0.2983 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.2671 - val_sparse_categorical_accuracy: 0.9180\n",
            "training client 2 's cnn\n",
            "282/282 [==============================] - 5s 11ms/step - loss: 0.2941 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.1745 - val_sparse_categorical_accuracy: 0.9555\n",
            "trained all clients cnn round 2\n",
            "Weighted aggregated client weights\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.2496 - sparse_categorical_accuracy: 0.9256\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9265\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.2532 - sparse_categorical_accuracy: 0.9260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_k = x_train[:no_sample]\n",
        "y_train_k = y_train[:no_sample]\n",
        "client = Client(smallCNN(), x_train_k, y_train_k)\n",
        "client.local_train(epochs=3)\n",
        "client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clXt_GxJD99-",
        "outputId": "9c82cf31-09b6-4fc8-da57-1822796ea9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "282/282 [==============================] - 5s 14ms/step - loss: 0.7805 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.3458 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 2/3\n",
            "282/282 [==============================] - 3s 12ms/step - loss: 0.3653 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.2914 - val_sparse_categorical_accuracy: 0.9185\n",
            "Epoch 3/3\n",
            "282/282 [==============================] - 3s 10ms/step - loss: 0.3179 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.2633 - val_sparse_categorical_accuracy: 0.9245\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2903 - sparse_categorical_accuracy: 0.9135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29031461477279663, 0.9135000109672546]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment: model quickly overfit to bad training data even after syncing weights, need to prevent this"
      ],
      "metadata": {
        "id": "Q5h6K_slalno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just Pruning (Don't Touch)"
      ],
      "metadata": {
        "id": "5nKDiEwT-ayP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client(smallCNN(), x_train_k, y_train_k))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 1, algo = 'FedPKDG')\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('trained all clients cnn round', _)\n",
        "  Server.agg_cnn()\n",
        "  print('Weighted aggregated client weights')\n",
        "\n",
        "for client in client_list:\n",
        "  get_model_sparsity(client.cnn)\n",
        "  client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSmgBFBa-lYt",
        "outputId": "689400d5-ee9b-478b-ee69-0a9220d096f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "after broadcasting client has sparsity 0.0056595559425337396\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 9s 13ms/step - loss: 0.8240 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.3441 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 9s 18ms/step - loss: 0.8242 - sparse_categorical_accuracy: 0.7551 - val_loss: 0.4127 - val_sparse_categorical_accuracy: 0.8890\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 12ms/step - loss: 0.8215 - sparse_categorical_accuracy: 0.7543 - val_loss: 0.2855 - val_sparse_categorical_accuracy: 0.9250\n",
            "trained all clients cnn round 0\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "after broadcasting client has sparsity 0.4264257727470614\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 12ms/step - loss: 0.3899 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.3071 - val_sparse_categorical_accuracy: 0.9175\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 14ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.3652 - val_sparse_categorical_accuracy: 0.8970\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3844 - sparse_categorical_accuracy: 0.8828 - val_loss: 0.2386 - val_sparse_categorical_accuracy: 0.9335\n",
            "trained all clients cnn round 1\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "after broadcasting client has sparsity 0.46408358728776666\n",
            "Broadcasted weights to all clients\n",
            "training client 0 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3507 - sparse_categorical_accuracy: 0.8952 - val_loss: 0.2816 - val_sparse_categorical_accuracy: 0.9190\n",
            "training client 1 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 8s 16ms/step - loss: 0.3512 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.3383 - val_sparse_categorical_accuracy: 0.9000\n",
            "training client 2 's cnn\n",
            "begin_step= 0 end_step= 282\n",
            "282/282 [==============================] - 7s 13ms/step - loss: 0.3445 - sparse_categorical_accuracy: 0.8972 - val_loss: 0.2243 - val_sparse_categorical_accuracy: 0.9370\n",
            "trained all clients cnn round 2\n",
            "before agg client 0 has sparsity 0.4849804092294297\n",
            "before agg client 1 has sparsity 0.4849804092294297\n",
            "before agg client 2 has sparsity 0.4849804092294297\n",
            "Weighted aggregated client weights\n",
            "313/313 [==============================] - 2s 5ms/step - loss: 0.3172 - sparse_categorical_accuracy: 0.9082\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3139 - sparse_categorical_accuracy: 0.9087\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 0.3113 - sparse_categorical_accuracy: 0.9087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_k = x_train[:no_sample]\n",
        "y_train_k = y_train[:no_sample]\n",
        "client = Client(smallCNN(), x_train_k, y_train_k)\n",
        "client.local_train(epochs = 3, is_prune = True)\n",
        "client.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nikIXLJS_NO_",
        "outputId": "62f42ea6-d9f9-4e5f-e359-2cfa16ac0df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin_step= 0 end_step= 844\n",
            "Epoch 1/3\n",
            "282/282 [==============================] - 7s 12ms/step - loss: 0.8385 - sparse_categorical_accuracy: 0.7484 - val_loss: 0.3486 - val_sparse_categorical_accuracy: 0.8985\n",
            "Epoch 2/3\n",
            "282/282 [==============================] - 4s 15ms/step - loss: 0.3786 - sparse_categorical_accuracy: 0.8857 - val_loss: 0.2972 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 3/3\n",
            "282/282 [==============================] - 3s 10ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.2797 - val_sparse_categorical_accuracy: 0.9235\n",
            "313/313 [==============================] - 2s 3ms/step - loss: 0.3094 - sparse_categorical_accuracy: 0.9068\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30936574935913086, 0.9067999720573425]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script for FedPKDG"
      ],
      "metadata": {
        "id": "R7gwXdOu4HjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate 3 clients\n",
        "no_sample = len(x_train) // 3\n",
        "client_list = []\n",
        "for i in range(3):\n",
        "  #partition dataset to mimic private data\n",
        "  x_train_k = x_train[no_sample*i:no_sample*(i+1)]\n",
        "  y_train_k = y_train[no_sample*i:no_sample*(i+1)]\n",
        "  client_list.append(Client(smallCNN(), x_train_k, y_train_k))\n",
        "\n",
        "#initiate 1 server\n",
        "Server = Server(smallCNN(), client_list, comm_freq = 2, algo='FedPKDG')\n",
        "\n",
        "for _ in range(3):\n",
        "  Server.broadcast()\n",
        "  print('>>>>>>>>>Broadcasted weights to all clients')\n",
        "  Server.local_training()\n",
        "  print('>>>>>>>>>trained all clients cnn round', _)\n",
        "  Server.agg_gen()\n",
        "  print('>>>>>>>>>Weighted aggregated client generator')\n",
        "  Server.produce_pseudo_dataset()\n",
        "  print('>>>>>>>>>produced pseudo data')\n",
        "  Server.distill_to_global()\n",
        "  print('>>>>>>>>>Knowledge distilled from clients and updated global weights')\n",
        "\n",
        "Server.broadcast()\n",
        "print('>>>>>>>>>Broadcasted weights to all clients')\n",
        "Server.local_training()\n",
        "print('>>>>>>>>>trained all clients cnn final round')\n",
        "for client in client_list:\n",
        "  print('local models evaluation')\n",
        "  client.cnn.evaluate(x_test, y_test)\n",
        "print('global model evaluation')\n",
        "Server.cnn.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q3SZD-W34F8y",
        "outputId": "759c1ac2-5714-4cb4-ed3b-1237de6b3b37"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>>>>>>>Broadcasted weights to all clients\n",
            "training client 0 's CNN\n",
            "from gloabl - before pruning client has sparsity 0.0056595559425337396\n",
            "begin_step= 0 end_step= 563\n",
            "Epoch 1/2\n",
            "282/282 [==============================] - 5s 8ms/step - loss: 0.7881 - sparse_categorical_accuracy: 0.7759 - val_loss: 0.3516 - val_sparse_categorical_accuracy: 0.9005\n",
            "Epoch 2/2\n",
            "282/282 [==============================] - 3s 10ms/step - loss: 0.3605 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.2912 - val_sparse_categorical_accuracy: 0.9160\n",
            "after pruning client has sparsity 0.3972572921201567\n",
            "training client 0 's GEN\n",
            "Epoch 1/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 34ms/step - d_loss: 0.4999 - g_loss: 1.2612\n",
            "Epoch 2/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 23ms/step - d_loss: 0.3047 - g_loss: 1.8706\n",
            "training client 1 's CNN\n",
            "from gloabl - before pruning client has sparsity 0.0056595559425337396\n",
            "begin_step= 0 end_step= 563\n",
            "Epoch 1/2\n",
            "282/282 [==============================] - 6s 7ms/step - loss: 0.7932 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.3873 - val_sparse_categorical_accuracy: 0.8825\n",
            "Epoch 2/2\n",
            "282/282 [==============================] - 2s 8ms/step - loss: 0.3529 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.3235 - val_sparse_categorical_accuracy: 0.9060\n",
            "after pruning client has sparsity 0.3972572921201567\n",
            "training client 1 's GEN\n",
            "Epoch 1/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - d_loss: 0.4462 - g_loss: 1.4131\n",
            "Epoch 2/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 24ms/step - d_loss: 0.3522 - g_loss: 1.6265\n",
            "training client 2 's CNN\n",
            "from gloabl - before pruning client has sparsity 0.0056595559425337396\n",
            "begin_step= 0 end_step= 563\n",
            "Epoch 1/2\n",
            "282/282 [==============================] - 5s 7ms/step - loss: 0.7836 - sparse_categorical_accuracy: 0.7848 - val_loss: 0.2598 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 2/2\n",
            "282/282 [==============================] - 2s 6ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8926 - val_loss: 0.2221 - val_sparse_categorical_accuracy: 0.9385\n",
            "after pruning client has sparsity 0.3972572921201567\n",
            "training client 2 's GEN\n",
            "Epoch 1/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - d_loss: 0.4790 - g_loss: 1.3673\n",
            "Epoch 2/2\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 24ms/step - d_loss: 0.3411 - g_loss: 1.6380\n",
            ">>>>>>>>>trained all clients cnn round 0\n",
            "----getting weighted avg of clients' gen\n",
            ">>>>>>>>>Weighted aggregated client generator\n",
            "Generating 10 fake images of digit 0 ......\n",
            "Generating 10 fake images of digit 1 ......\n",
            "Generating 10 fake images of digit 2 ......\n",
            "Generating 10 fake images of digit 3 ......\n",
            "Generating 10 fake images of digit 4 ......\n",
            "Generating 10 fake images of digit 5 ......\n",
            "Generating 10 fake images of digit 6 ......\n",
            "Generating 10 fake images of digit 7 ......\n",
            "Generating 10 fake images of digit 8 ......\n",
            "Generating 10 fake images of digit 9 ......\n",
            ">>>>>>>>>produced pseudo data\n",
            "----getting weighted avg of clients' classifier\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"<ipython-input-9-7b430067f1f7>\", line 17, in train_step  *\n        x, y = data\n\n    ValueError: not enough values to unpack (expected 2, got 1)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2da6540de101>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mServer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduce_pseudo_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>>>>>>>>produced pseudo data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mServer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistill_to_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>>>>>>>>Knowledge distilled from clients and updated global weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-c23572149ea9>\u001b[0m in \u001b[0;36mdistill_to_global\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Distill teacher to student\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'should print not trainable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit_compile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'run_step'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_per_replica\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_reduction_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     16\u001b[0m                         \u001b[0mdo_return_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mretval__1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                             \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filec3a7qwzx.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mteacher_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_teacher_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step  *\n        outputs = model.train_step(data)\n    File \"<ipython-input-9-7b430067f1f7>\", line 17, in train_step  *\n        x, y = data\n\n    ValueError: not enough values to unpack (expected 2, got 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Server.public_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXiGdiIvKA2R",
        "outputId": "481dcfc6-520c-44b3-ceaa-241bc94c179c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[[106],\n",
              "          [ 80],\n",
              "          [ 56],\n",
              "          ...,\n",
              "          [  7],\n",
              "          [ 13],\n",
              "          [ 21]],\n",
              " \n",
              "         [[ 85],\n",
              "          [ 59],\n",
              "          [ 23],\n",
              "          ...,\n",
              "          [  2],\n",
              "          [  3],\n",
              "          [ 12]],\n",
              " \n",
              "         [[ 93],\n",
              "          [ 65],\n",
              "          [ 27],\n",
              "          ...,\n",
              "          [  5],\n",
              "          [  3],\n",
              "          [ 10]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 14],\n",
              "          [  3],\n",
              "          [  1],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 24],\n",
              "          [  7],\n",
              "          [  2],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 47],\n",
              "          [ 24],\n",
              "          [ 17],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  2]]],\n",
              " \n",
              " \n",
              "        [[[ 53],\n",
              "          [ 19],\n",
              "          [  9],\n",
              "          ...,\n",
              "          [  6],\n",
              "          [ 13],\n",
              "          [ 22]],\n",
              " \n",
              "         [[ 79],\n",
              "          [ 37],\n",
              "          [  7],\n",
              "          ...,\n",
              "          [  3],\n",
              "          [  4],\n",
              "          [ 11]],\n",
              " \n",
              "         [[146],\n",
              "          [ 68],\n",
              "          [ 20],\n",
              "          ...,\n",
              "          [  5],\n",
              "          [  4],\n",
              "          [  6]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[  0],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[  3],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 25],\n",
              "          [  5],\n",
              "          [  2],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]]],\n",
              " \n",
              " \n",
              "        [[[ 46],\n",
              "          [ 35],\n",
              "          [ 16],\n",
              "          ...,\n",
              "          [  1],\n",
              "          [  5],\n",
              "          [ 11]],\n",
              " \n",
              "         [[ 23],\n",
              "          [ 14],\n",
              "          [  3],\n",
              "          ...,\n",
              "          [  1],\n",
              "          [  2],\n",
              "          [  6]],\n",
              " \n",
              "         [[ 28],\n",
              "          [ 28],\n",
              "          [  4],\n",
              "          ...,\n",
              "          [  4],\n",
              "          [  4],\n",
              "          [  4]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[136],\n",
              "          [210],\n",
              "          [222],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[118],\n",
              "          [105],\n",
              "          [ 15],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[  4],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]]],\n",
              " \n",
              " \n",
              "        ...,\n",
              " \n",
              " \n",
              "        [[[ 36],\n",
              "          [ 22],\n",
              "          [ 14],\n",
              "          ...,\n",
              "          [  1],\n",
              "          [  3],\n",
              "          [  8]],\n",
              " \n",
              "         [[ 21],\n",
              "          [ 14],\n",
              "          [  8],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  1],\n",
              "          [  6]],\n",
              " \n",
              "         [[ 24],\n",
              "          [ 25],\n",
              "          [ 17],\n",
              "          ...,\n",
              "          [ 10],\n",
              "          [  6],\n",
              "          [  8]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[  8],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  7],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 19],\n",
              "          [  2],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 50],\n",
              "          [ 16],\n",
              "          [  2],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]]],\n",
              " \n",
              " \n",
              "        [[[  4],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [ 27],\n",
              "          [ 29],\n",
              "          [ 35]],\n",
              " \n",
              "         [[  0],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [ 14],\n",
              "          [ 15],\n",
              "          [ 27]],\n",
              " \n",
              "         [[  0],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  8],\n",
              "          [  8],\n",
              "          [ 15]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[  0],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[  1],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 13],\n",
              "          [  1],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]]],\n",
              " \n",
              " \n",
              "        [[[  6],\n",
              "          [  0],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  9],\n",
              "          [ 16],\n",
              "          [ 26]],\n",
              " \n",
              "         [[ 30],\n",
              "          [  4],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  5],\n",
              "          [  7],\n",
              "          [ 17]],\n",
              " \n",
              "         [[ 63],\n",
              "          [  3],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [ 10],\n",
              "          [ 10],\n",
              "          [ 14]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 32],\n",
              "          [  3],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 18],\n",
              "          [  2],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]],\n",
              " \n",
              "         [[ 12],\n",
              "          [  2],\n",
              "          [  0],\n",
              "          ...,\n",
              "          [  0],\n",
              "          [  0],\n",
              "          [  0]]]], dtype=uint8),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
              "        6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "        8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "my58ZMvu-l3U"
      }
    }
  ]
}